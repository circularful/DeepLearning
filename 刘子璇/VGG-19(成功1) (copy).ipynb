{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import what we need\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf  # Import TensorFlow after Scipy or Scipy will break\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出图片的路径\n",
    "OUTPUT_DIR = '/home/liuzixuan/实训/02-用Python快速实现图片的风格迁移/output_image'\n",
    "# 风格图片的路径\n",
    "STYLE_IMAGE = '/home/liuzixuan/实训/02-用Python快速实现图片的风格迁移/img/star.jpg'\n",
    "# 内容图片的路径\n",
    "CONTENT_IMAGE = '/home/liuzixuan/实训/02-用Python快速实现图片的风格迁移/img'\n",
    "# VGG-19模型的路径\n",
    "VGG_MODEL = '/home/liuzixuan/实训/02-用Python快速实现图片的风格迁移/刘子璇/imagenet-vgg-verydeep-19.mat'\n",
    "# 模型存取路径\n",
    "MODEL = '/home/liuzixuan/实训/02-用Python快速实现图片的风格迁移/刘子璇/model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所需常数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出图片大小定义(224,224,3)\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "COLOR_CHANNELS = 3\n",
    "# 噪声比(与内容图像混合的噪声权重百分比)\n",
    "NOISE_RATIO = 0.6\n",
    "# Constant to put more emphasis on content loss.\n",
    "BETA = 5\n",
    "# Constant to put more emphasis on style loss.\n",
    "ALPHA = 100\n",
    "# 从VGG模型的输入中减去的平均值\n",
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "# 训练次数\n",
    "ITERATIONS = 2000 \n",
    "\n",
    "STYLE_WEIGHT = 1\n",
    "CONTENT_WEIGHT = 1\n",
    "STYLE_LAYERS = ['relu1_2','relu2_2','relu3_2']\n",
    "CONTENT_LAYERS = ['relu1_2']\n",
    "_vgg_params = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg_model(path):\n",
    "    \"\"\"\n",
    "    Returns a model for the purpose of 'painting' the picture.\n",
    "    Takes only the convolution layer weights and wrap using the TensorFlow\n",
    "    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but\n",
    "    the paper indicates that using AveragePooling yields better results.\n",
    "    The last few fully connected layers are not used.\n",
    "    Here is the detailed configuration of the VGG model:\n",
    "        0 is conv1_1 (3, 3, 3, 64)\n",
    "        1 is relu\n",
    "        2 is conv1_2 (3, 3, 64, 64)\n",
    "        3 is relu    \n",
    "        4 is maxpool\n",
    "        5 is conv2_1 (3, 3, 64, 128)\n",
    "        6 is relu\n",
    "        7 is conv2_2 (3, 3, 128, 128)\n",
    "        8 is relu\n",
    "        9 is maxpool\n",
    "        10 is conv3_1 (3, 3, 128, 256)\n",
    "        11 is relu\n",
    "        12 is conv3_2 (3, 3, 256, 256)\n",
    "        13 is relu\n",
    "        14 is conv3_3 (3, 3, 256, 256)\n",
    "        15 is relu\n",
    "        16 is conv3_4 (3, 3, 256, 256)\n",
    "        17 is relu\n",
    "        18 is maxpool\n",
    "        19 is conv4_1 (3, 3, 256, 512)\n",
    "        20 is relu\n",
    "        21 is conv4_2 (3, 3, 512, 512)\n",
    "        22 is relu\n",
    "        23 is conv4_3 (3, 3, 512, 512)\n",
    "        24 is relu\n",
    "        25 is conv4_4 (3, 3, 512, 512)\n",
    "        26 is relu\n",
    "        27 is maxpool\n",
    "        28 is conv5_1 (3, 3, 512, 512)\n",
    "        29 is relu\n",
    "        30 is conv5_2 (3, 3, 512, 512)\n",
    "        31 is relu\n",
    "        32 is conv5_3 (3, 3, 512, 512)\n",
    "        33 is relu\n",
    "        34 is conv5_4 (3, 3, 512, 512)\n",
    "        35 is relu\n",
    "        36 is maxpool\n",
    "        37 is fullyconnected (7, 7, 512, 4096)\n",
    "        38 is relu\n",
    "        39 is fullyconnected (1, 1, 4096, 4096)\n",
    "        40 is relu\n",
    "        41 is fullyconnected (1, 1, 4096, 1000)\n",
    "        42 is softmax\n",
    "    \"\"\"\n",
    "    vgg = scipy.io.loadmat(path)\n",
    "\n",
    "    vgg_layers = vgg['layers']\n",
    "    # 定义权重函数(返回各层权重及偏置值)\n",
    "    def _weights(layer, expected_layer_name):\n",
    "        W = vgg_layers[0][layer][0][0][0][0][0]\n",
    "        b = vgg_layers[0][layer][0][0][0][0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][-2]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "    # 定义各层函数\n",
    "    def _relu(conv2d_layer):\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name):\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(\n",
    "            prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _avgpool(prev_layer):\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 构造图形模型\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内容损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(target_features,content_features):\n",
    "    # 使用特征图之差的平方和作为内容差距，越小则合成图与原图的内容越接近\n",
    "    _,height,width,channel = map(lambda i:i.value,content_features.get_shape())\n",
    "    #print ('content_features.get_shape() : ')\n",
    "    #print (content_features.get_shape())\n",
    "    content_size = height * width * channel\n",
    "    return tf.nn.l2_loss(target_features - content_features) / content_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 风格损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(target_features,style_features):\n",
    "    # 使用Gram matrix之差的平方和作为风格差距，越小则合成图像越具有风格图的纹理特征\n",
    "    _,height,width,channel = map(lambda i:i.value,target_features.get_shape())\n",
    "    #print ('target_features.get_shape() : ')\n",
    "    #print (target_features.get_shape())\n",
    "    size = height * width * channel\n",
    "    #targer gram 是特征图矩阵的内积\n",
    "    target_features = tf.reshape(target_features,(-1,channel))\n",
    "    target_gram = tf.matmul(tf.transpose(target_features),target_features) / size\n",
    "\n",
    "    style_features = tf.reshape(style_features,(-1,channel))\n",
    "    style_gram = tf.matmul(tf.transpose(style_features),style_features) / size\n",
    "\n",
    "    return tf.nn.l2_loss(target_gram - style_gram) / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(style_image,content_image,target_image):\n",
    "    #style_features = vgg19([style_image])\n",
    "    #content_features = vgg19([content_image])\n",
    "    #target_features = vgg19([target_image])\n",
    "    style_features = model['input'].assign(style_image)\n",
    "    content_features = model['input'].assign(content_image)\n",
    "    target_features = model['input'].assign(target_image)\n",
    "    #content_loss = content_loss(target_features=target_features,content_features=content_features)\n",
    "    #style_loss = style_loss(target_features=target_features,style_features=style_features)\n",
    "    #loss = 0.0\n",
    "    #for layer in CONTENT_LAYERS:\n",
    "    #    loss +=  CONTENT_WEIGHT * content_loss(target_features[layer],content_features[layer])\n",
    "\n",
    "    #for layer in STYLE_LAYERS:\n",
    "    #    loss +=  STYLE_WEIGHT * style_loss(target_features[layer],style_features[layer])\n",
    "    \n",
    "    loss = BETA * content_loss(target_features,content_features) + ALPHA * style_loss(target_features,style_features)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8fcd3783c71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 风格图片\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTYLE_IMAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#imshow(style_image[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 模型加载\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_image' is not defined"
     ]
    }
   ],
   "source": [
    "# 风格图片\n",
    "style_image = load_image(STYLE_IMAGE)\n",
    "#imshow(style_image[0])\n",
    "    \n",
    "# 模型加载\n",
    "model = load_vgg_model(VGG_MODEL)\n",
    "with tf.Session() as sess:\n",
    "    style_features = model['input'].assign(style_image)\n",
    "    sess.run(style_features)\n",
    "    print(style_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'style_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7355742707e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'style_image' is not defined"
     ]
    }
   ],
   "source": [
    "print(style_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 噪声图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise_image(content_image, noise_ratio = NOISE_RATIO):\n",
    "    # 噪声图像:与内容图像以一定比例混合\n",
    "    noise_image = np.random.uniform(\n",
    "            -20, 20,\n",
    "            (1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)).astype('float32')\n",
    "    # 加权平均值\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = scipy.misc.imread(path)\n",
    "    # 统一图片大小(1,224,224,3)\n",
    "    image = scipy.misc.imresize(image,(224,224,3))\n",
    "    image = np.reshape(image, ((1,) + image.shape))\n",
    "    # VGG模型的输入需要减去平均值\n",
    "    image = image - MEAN_VALUES\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(path, image):\n",
    "    # 输出图片需要加回平均值\n",
    "    image = image + MEAN_VALUES\n",
    "    # 去掉第一个无用的维度，剩下的就是图像\n",
    "    image = image[0]\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    style = tf.placeholder(shape=[None,224,224,3],dtype=tf.float32)\n",
    "    content = tf.placeholder(shape=[None,224,224,3],dtype=tf.float32)\n",
    "    target = tf.placeholder(shape=[None,224,224,3],dtype=tf.float32)\n",
    "    # 模型加载\n",
    "    model = load_vgg_model(VGG_MODEL)\n",
    "    loss = loss_function(style,content,target)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ba1a80585a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 优化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#rain_op = optimizer.minimize(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    # 优化器\n",
    "    optimizer = tf.train.AdamOptimizer(2.0)\n",
    "    #rain_op = optimizer.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    style_image = load_image(STYLE_IMAGE)\n",
    "    content_image = load_image(CONTENT_IMAGE+'/'+'img0'+str(1)+'.jpg')\n",
    "    target_image = generate_noise_image(content_image)\n",
    "    print(type(style_image))\n",
    "    # 训练模型\n",
    "    for step in range(ITERATIONS):\n",
    "        #oss,_ = sess.run([loss,train_op],feed_dict={style:style_image,content:content_image,target:target_image})\n",
    "        loss=sess.run(loss,feed_dict={style:style_image,content:content_image,target:target_image})\n",
    "        if step % 200 == 0:\n",
    "            mixed_image = sess.run(model['input'])\n",
    "            print('step %5d, loss %2.4f' % (step, loss))\n",
    "            filename = OUTPUT_DIR + '/' + 'img0' + str(step)+'.jpg'\n",
    "            saver.save(sess, MODEL+'model'+str(step/200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzixuan/test/lib/python3.5/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/home/liuzixuan/test/lib/python3.5/site-packages/ipykernel_launcher.py:4: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img0 1\n",
      "Iteration 0\n",
      "sum :  297664.7\n",
      "cost:  143579330000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzixuan/test/lib/python3.5/site-packages/ipykernel_launcher.py:7: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n",
      "sum :  -40959.406\n",
      "cost:  1989663400.0\n",
      "Iteration 200\n",
      "sum :  -232078.22\n",
      "cost:  1067706900.0\n",
      "Iteration 300\n",
      "sum :  -396636.62\n",
      "cost:  746889300.0\n",
      "Iteration 400\n",
      "sum :  -556374.4\n",
      "cost:  566964160.0\n",
      "Iteration 500\n",
      "sum :  -712374.5\n",
      "cost:  453271460.0\n",
      "Iteration 600\n",
      "sum :  -864302.4\n",
      "cost:  373953730.0\n",
      "Iteration 700\n",
      "sum :  -1008806.2\n",
      "cost:  318392770.0\n",
      "Iteration 800\n",
      "sum :  -1145272.8\n",
      "cost:  278326660.0\n",
      "Iteration 900\n",
      "sum :  -1274023.5\n",
      "cost:  248263150.0\n",
      "Iteration 1000\n",
      "sum :  -1397839.0\n",
      "cost:  224924670.0\n",
      "Iteration 1100\n",
      "sum :  -1515923.8\n",
      "cost:  209082740.0\n",
      "Iteration 1200\n",
      "sum :  -1628693.0\n",
      "cost:  192290450.0\n",
      "Iteration 1300\n",
      "sum :  -1736330.1\n",
      "cost:  182231790.0\n",
      "Iteration 1400\n",
      "sum :  -1837116.2\n",
      "cost:  192580260.0\n",
      "Iteration 1500\n",
      "sum :  -1930873.2\n",
      "cost:  160938400.0\n",
      "Iteration 1600\n",
      "sum :  -2020672.1\n",
      "cost:  152959060.0\n",
      "Iteration 1700\n",
      "sum :  -2103264.0\n",
      "cost:  145366820.0\n",
      "Iteration 1800\n",
      "sum :  -2182016.2\n",
      "cost:  141120380.0\n",
      "Iteration 1900\n",
      "sum :  -2256179.0\n",
      "cost:  141778480.0\n",
      "###########################################################################\n",
      "img0 2\n",
      "Iteration 0\n",
      "sum :  -2311367.0\n",
      "cost:  137978450000.0\n",
      "Iteration 100\n",
      "sum :  -2432873.0\n",
      "cost:  2665815300.0\n",
      "Iteration 200\n",
      "sum :  -2542479.5\n",
      "cost:  1259732700.0\n",
      "Iteration 300\n",
      "sum :  -2637176.0\n",
      "cost:  790757100.0\n",
      "Iteration 400\n",
      "sum :  -2724160.5\n",
      "cost:  566786940.0\n",
      "Iteration 500\n",
      "sum :  -2802031.8\n",
      "cost:  438031870.0\n",
      "Iteration 600\n",
      "sum :  -2872032.0\n",
      "cost:  358964060.0\n",
      "Iteration 700\n",
      "sum :  -2934151.5\n",
      "cost:  307137800.0\n",
      "Iteration 800\n",
      "sum :  -2990534.0\n",
      "cost:  270619970.0\n",
      "Iteration 900\n",
      "sum :  -3041679.0\n",
      "cost:  244079260.0\n",
      "Iteration 1000\n",
      "sum :  -3088753.2\n",
      "cost:  222403470.0\n",
      "Iteration 1100\n",
      "sum :  -3133651.5\n",
      "cost:  209413840.0\n",
      "Iteration 1200\n",
      "sum :  -3176027.5\n",
      "cost:  201399540.0\n",
      "Iteration 1300\n",
      "sum :  -3215060.5\n",
      "cost:  184801090.0\n",
      "Iteration 1400\n",
      "sum :  -3250421.0\n",
      "cost:  222889060.0\n",
      "Iteration 1500\n",
      "sum :  -3282285.2\n",
      "cost:  164672300.0\n",
      "Iteration 1600\n",
      "sum :  -3310650.2\n",
      "cost:  157932590.0\n",
      "Iteration 1700\n",
      "sum :  -3336473.0\n",
      "cost:  153185040.0\n",
      "Iteration 1800\n",
      "sum :  -3360464.5\n",
      "cost:  250961660.0\n",
      "Iteration 1900\n",
      "sum :  -3381311.2\n",
      "cost:  142693570.0\n",
      "###########################################################################\n",
      "img0 3\n",
      "Iteration 0\n",
      "sum :  -2811616.0\n",
      "cost:  125908050000.0\n",
      "Iteration 100\n",
      "sum :  -3060895.5\n",
      "cost:  1657189500.0\n",
      "Iteration 200\n",
      "sum :  -3143493.0\n",
      "cost:  795610940.0\n",
      "Iteration 300\n",
      "sum :  -3224591.2\n",
      "cost:  537080900.0\n",
      "Iteration 400\n",
      "sum :  -3297083.5\n",
      "cost:  409059400.0\n",
      "Iteration 500\n",
      "sum :  -3361245.8\n",
      "cost:  331608580.0\n",
      "Iteration 600\n",
      "sum :  -3416600.5\n",
      "cost:  280347420.0\n",
      "Iteration 700\n",
      "sum :  -3464279.0\n",
      "cost:  244199570.0\n",
      "Iteration 800\n",
      "sum :  -3507270.2\n",
      "cost:  217626460.0\n",
      "Iteration 900\n",
      "sum :  -3544804.5\n",
      "cost:  198004530.0\n",
      "Iteration 1000\n",
      "sum :  -3578073.2\n",
      "cost:  183106770.0\n",
      "Iteration 1100\n",
      "sum :  -3608221.5\n",
      "cost:  175118780.0\n",
      "Iteration 1200\n",
      "sum :  -3634658.0\n",
      "cost:  164794580.0\n",
      "Iteration 1300\n",
      "sum :  -3658098.5\n",
      "cost:  154376400.0\n",
      "Iteration 1400\n",
      "sum :  -3678848.2\n",
      "cost:  144834670.0\n",
      "Iteration 1500\n",
      "sum :  -3697302.5\n",
      "cost:  139564540.0\n",
      "Iteration 1600\n",
      "sum :  -3713187.0\n",
      "cost:  152286320.0\n",
      "Iteration 1700\n",
      "sum :  -3726048.5\n",
      "cost:  128618104.0\n",
      "Iteration 1800\n",
      "sum :  -3736084.8\n",
      "cost:  133193850.0\n",
      "Iteration 1900\n",
      "sum :  -3744498.0\n",
      "cost:  120876050.0\n",
      "###########################################################################\n",
      "img0 4\n",
      "Iteration 0\n",
      "sum :  -1758278.8\n",
      "cost:  97928905000.0\n",
      "Iteration 100\n",
      "sum :  -1914953.8\n",
      "cost:  1539196900.0\n",
      "Iteration 200\n",
      "sum :  -2013217.2\n",
      "cost:  804013800.0\n",
      "Iteration 300\n",
      "sum :  -2105039.8\n",
      "cost:  536955900.0\n",
      "Iteration 400\n",
      "sum :  -2192021.8\n",
      "cost:  405195900.0\n",
      "Iteration 500\n",
      "sum :  -2275242.0\n",
      "cost:  326202780.0\n",
      "Iteration 600\n",
      "sum :  -2353960.0\n",
      "cost:  274611460.0\n",
      "Iteration 700\n",
      "sum :  -2426186.0\n",
      "cost:  238439870.0\n",
      "Iteration 800\n",
      "sum :  -2492948.2\n",
      "cost:  212333200.0\n",
      "Iteration 900\n",
      "sum :  -2555257.0\n",
      "cost:  195267360.0\n",
      "Iteration 1000\n",
      "sum :  -2613116.8\n",
      "cost:  180264200.0\n",
      "Iteration 1100\n",
      "sum :  -2666323.5\n",
      "cost:  167094290.0\n",
      "Iteration 1200\n",
      "sum :  -2713907.5\n",
      "cost:  170322140.0\n",
      "Iteration 1300\n",
      "sum :  -2757688.0\n",
      "cost:  145526100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5ffff3339abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# 每迭代１００次输出一次\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #优化器\n",
    "    #optimizer = tf.train.AdamOptimizer(2.0)\n",
    "    #train_step = optimizer.minimize(total_loss)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    # 训练模型\n",
    "    \n",
    "    #for step in range(1000):\n",
    "    #    #batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "    #    \n",
    "    #    res_loss, _ = sess.run([total_loss, train_step], feed_dict={\n",
    "    #        content: content_image,\n",
    "    #        style: style_image\n",
    "    #    })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "    #    if step % 100 == 0:\n",
    "    #        accs = []\n",
    "    #        for test_step in range(10000 // 32):\n",
    "    #            batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "    #            res_acc = sess.run(acc, feed_dict={\n",
    "    #                inputs: batch_images,\n",
    "    #                labels: batch_labels\n",
    "    #            })\n",
    "    #           accs.append(res_acc)\n",
    "    #        accs = np.mean(accs)\n",
    "    #        print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))\n",
    "    \n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #saver = tf.train.Saver()\n",
    "    # 风格图片\n",
    "    style_image = load_image(STYLE_IMAGE)\n",
    "    #imshow(style_image[0])\n",
    "\n",
    "    # 模型加载\n",
    "    model = load_vgg_model(VGG_MODEL)\n",
    "    #print(model)\n",
    "\n",
    "    #print(content_image.shape)\n",
    "    #print(style_image.shape)\n",
    "    for i  in range(1,6):\n",
    "        # 内容图片\n",
    "        content_image = load_image(CONTENT_IMAGE+'/'+'img0'+str(i)+'.jpg')\n",
    "        #imshow(content_image[0])\n",
    "    \n",
    "        # 输入图片\n",
    "        input_image = generate_noise_image(content_image)\n",
    "        #imshow(input_image[0])\n",
    "    \n",
    "        # 内容损失\n",
    "        sess.run(model['input'].assign(content_image))\n",
    "        content_loss = content_loss_func(sess, model)\n",
    "    \n",
    "        # 风格损失\n",
    "        sess.run(model['input'].assign(style_image))\n",
    "        style_loss = style_loss_func(sess, model)\n",
    "    \n",
    "        #总损失\n",
    "        total_loss = BETA * content_loss + ALPHA * style_loss\n",
    "    \n",
    "        #优化器\n",
    "        optimizer = tf.train.AdamOptimizer(2.0)\n",
    "        train_step = optimizer.minimize(total_loss)\n",
    "        \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        sess.run(model['input'].assign(input_image))\n",
    "    #    sess.run(tf.initialize_all_variables())\n",
    "    #    sess.run(model['input'].assign(input_image))\n",
    "        print('img0',i)\n",
    "        for j in range(ITERATIONS):\n",
    "            sess.run(train_step)\n",
    "            # 每迭代１００次输出一次\n",
    "            if j%100 == 0:\n",
    "                #tf.train.Saver.save(sess=sess,save_path=MODEL+'model_'+str(i),global_step=j)\n",
    "                mixed_image = sess.run(model['input'])\n",
    "                print('Iteration %d' % (j))\n",
    "                print('sum : ', sess.run(tf.reduce_sum(mixed_image)))\n",
    "                print('cost: ', sess.run(total_loss))\n",
    "\n",
    "                if not os.path.exists(OUTPUT_DIR):\n",
    "                    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "                filename = OUTPUT_DIR + '/' + 'img0'+ str(i)+'_'+ str(j) + '.jpg' \n",
    "                save_image(filename, mixed_image)\n",
    "        print('###########################################################################')\n",
    "        #saver.save(sess,'model'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Saver.save??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
